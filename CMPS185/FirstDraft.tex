\documentclass[12pt,twocolumn]{article}
\usepackage{url}
\usepackage{listings}
\usepackage{textcomp}
\usepackage{amsmath}
\usepackage[T1]{fontenc} % Turns quotes straight
\bibliographystyle{ieeetr}

\newcommand\blfootnote[1]{%
	\begingroup
	\renewcommand\thefootnote{}\footnote{#1}%
	\addtocounter{footnote}{-1}%
	\endgroup
}


\title{Quality-controlling real-time crowdsourced data \\ \large First Draft}

\author{Aaron Steele}
\date{}


\begin{document}
	\maketitle
	
	\blfootnote{Conference: UbiComp'11 (\url{https://dl.acm.org/citation.cfm?id=2030100&picked=prox}) }
	\blfootnote{Papers: \cite{UB-QC-CC}\cite{QC-CC}}
	
	% Outline
	% I. Quality controlling normal crowdsourced data [1st paper]
	%	1. Web-based crowdsourcing
	%	2. QC of standard crowdsourcing
	%	3. Taxonomy of quality in crowdsourcing systems
	%		1. Worker: Reputation & Expertise
	%		2. Task: Definition, User Interface, Granluarity, Compensation Policy
	%	4. Quality control approaches [table] (Contribuitor evaulation?)
	% II. Intro to real-time QC
	%	1. Definition of UB QC
	%	2. Problems with dynamic crowds and UB data
	%	3. Participatory sensing
	% III. The credibility-weight and proposed approach
	%	1. Using mobility patterns
	%	2. Defining POIs via tuples
	%	3. Regularity function (how regularly a user visits a place)
	%	4. Trustworthiness score, or reputation
	%	5. Final formula and explanation
	
	\section*{Abstract}
	    TODO
	\section*{Introduction}
	    Crowdsourcing has become a much more popular way of both getting and processing data in recent years. In processing data, crowdsourcing is primarily helpful for tasks which are easy for humans to do but hard to machines. %\cite
	    Getting data, on the other hand, is a very different process. With the advent of the ubiquity of smartphones, it has become much easier to gather various types of data from the crowd, as it were.
	    
	    Quality controlling crowdsourced processing is, in a lot of ways, easier than doing the same with crowdsourced data. We have created a taxonomy of quality in crowdsourced systems. There are various approaches to quality controlling the processed data, but we believe the most relevant one for our purposes is Contributor Evaluation, which is where a contribution is assessed based on the quality of the contributor's previous contributions. 
	    %Recreate and include the taxonomy diagram?
	    
	\section*{Body}
	    Thanks to the rise of smartphones being abundant, a new type of crowdsourcing has been created. Ubiquitous crowdsourcing is smartphone owners contributing data about their outside world, such as GPS location, or ambient noise level. For example, Google Maps, or Waze, both rely on crowdsourced data to give information about traffic conditions, wrecks, and other events that happen on the road. 
	    
	    A major issue that faces developers who wish to use ubiquitous crowdsourcing is quality control. Just like in crowdsourced processing, the data gathered from ubiquitous applications must be controlled for quality. For ubiquitous systems, quality control is even more important than processing systems, in a lot of cases. On top of dealing with outright malicious users submitting bad data, the "fuzziness" of the real world requires us to deal with a truly huge amount of possible edge cases. Getting usable data out of the mess is what this paper will be discussing.
	    
	    Two other issues that ubiquitous crowdsourcing faces that crowdsourced processing doesn't have to deal with are Real-time Events, and Dynamic Crowds.
	    \begin{itemize}
	        \item \emph{Real-time Events}: ubiquitous crowdsourcing inherently deals with real-time events. The data is gathered in real time, and in many instances, real-time processing of the data is expected as well. %Example? Bus stop and location logging?
	        In crowdsourced processing, on the other hand, quality control can often be delayed by some amount of time, if needed, to be checked and flagged by an authorized or more credible user. 
	        
	        
	        \item \emph{Dynamic Crowds}: since ubiquitous crowdsourcing deals in real-time, the crowd itself is also often dynamic. People start and stop driving all the time, for example. The challenge of a dynamic crowd is that there are times when the number of contributors might not reach critical mass. Having too few points of data makes quality control even more important, on top of increasing the challenge of getting the proper results from the program.
	    \end{itemize}
	    
	    Participatory sensing is the concept of communities (or other groups of people) contributing sensory information to form a body of knowledge. %\cite https://www.wilsoncenter.org/sites/default/files/participatory_sensing.pdf
	    A common problem with participatory sensing is data corruption, or malicious users intentionally sending invalid or fallacious data. The paper %cite [17]
	    [] shows a new concept of controlling for this by allowing consumers of the crowdsourced data to assign trust scores to specific sources of data. 
	    
	    The only other paper that is related to our work is []. %cite [8]
	    In that paper, the authors consider the scenario where users deliberately try to confuse the sensor and send false data. The solution the authors propose is using a trust-based rating system, where each contributor has a reputation, or trust, rating and processed the data sent from users based on this rating. While their system shows an improvement over other trust-based systems, we propose an improvement to that system.
	    	% III. The credibility-weight and proposed approach
	%	1. Using mobility patterns
	%	2. Defining POIs via tuples
	%	3. Regularity function (how regularly a user visits a place)
	%	4. Trustworthiness score, or reputation
	%	5. Final formula and explanation
	    
	    Our proposed method is to track the users' mobility patterns. Studies have shown that data can be very consistent when cross-referencing with a users' commute. For example, there is a high degree of regularity in a weekday commute; most people tend to make a schedule and stick to it when traveling. [] %Cite the article [18]
	    We look at taking a public bus, and users' traveling patterns to determine when the busses will arrive at given stops. First we define, for each user, Points of Interest (POIs) with tuples, $T(loc_{POI_x}, t_i)$. $loc_{POI}$ is the specific point where the contribution was submitted. $t_i$ is a logical time as opposed to a timestamp. We define a logical time as something applicatioin-relevant, such as \emph{morning}, \emph{afternoon}, or \emph{late night}.
	    
	    
	    
	    \\
	    The formula we created: 
	    $\text{credibility weight} (T_j) = \alpha \cdot Reg(T_j) + (1 - \alpha) \cdot Trust(u_i)$
	    
	    
	\section*{Related Works}
	    % Web-based crowdsourcing, or crowdsourced processing
	    % 
	    
	\section*{Conclusion}
	    TODO
	
	\bibliography{FirstDraft}
\end{document}